{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asuka4649/cv/blob/main/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "outputs": [],
      "source": [
        "path_to_file = '/SteveJobs_StanfordSpeech.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d70e6e-5f1c-4018-dd39-677cc96fb04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 11934 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='windows-1252')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41a0aaa-d5fe-442c-e95c-3cc68fc4b8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you. I’m honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college, and this is the closest I’ve ever gotten to a college graduation today. I want to tell \n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b757b3c-2106-4e30-842b-3c9279367a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d7251f-7739-4949-9a25-cc3d51fbda0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83c0292-14c1-4c6c-cef4-0207b5a00e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32196170-3a20-4148-c89c-39613d5674e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e86edb-a989-4fad-be01-fa05582d81c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb664a5c-527f-47ae-ccbd-92ff01f08e15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(11934,), dtype=int64, numpy=array([36, 47, 40, ...,  1,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fd2d59-b4ad-49bf-a1bf-459b9802d0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n",
            "h\n",
            "a\n",
            "n\n",
            "k\n",
            " \n",
            "y\n",
            "o\n",
            "u\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a22d9d-a0cc-4229-d956-6e70cb7fc99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'T' b'h' b'a' b'n' b'k' b' ' b'y' b'o' b'u' b'.' b' ' b'I'\n",
            " b'\\xe2\\x80\\x99' b'm' b' ' b'h' b'o' b'n' b'o' b'r' b'e' b'd' b' ' b't'\n",
            " b'o' b' ' b'b' b'e' b' ' b'w' b'i' b't' b'h' b' ' b'y' b'o' b'u' b' '\n",
            " b't' b'o' b'd' b'a' b'y' b' ' b'f' b'o' b'r' b' ' b'y' b'o' b'u' b'r'\n",
            " b' ' b'c' b'o' b'm' b'm' b'e' b'n' b'c' b'e' b'm' b'e' b'n' b't' b' '\n",
            " b'f' b'r' b'o' b'm' b' ' b'o' b'n' b'e' b' ' b'o' b'f' b' ' b't' b'h'\n",
            " b'e' b' ' b'f' b'i' b'n' b'e' b's' b't' b' ' b'u' b'n' b'i' b'v' b'e'\n",
            " b'r' b's' b'i' b't' b'i' b'e' b's'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118b2d14-d497-40af-ec1d-027c9b2e435a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Thank you. I\\xe2\\x80\\x99m honored to be with you today for your commencement from one of the finest universities'\n",
            "b' in the world. Truth be told, I never graduated from college, and this is the closest I\\xe2\\x80\\x99ve ever gotte'\n",
            "b'n to a college graduation today. I want to tell you three stories from my life. That\\xe2\\x80\\x99s it. No big dea'\n",
            "b'l. Just three stories. The first story is about connecting the dots. I dropped out of Reed College af'\n",
            "b'ter the first six months, but then stayed around as a drop-in for another 18 months or so before I re'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14173637-1055-4a70-b596-4869f041eec5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b801be4e-361d-4c0c-cba9-a7613230e670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'Thank you. I\\xe2\\x80\\x99m honored to be with you today for your commencement from one of the finest universitie'\n",
            "Target: b'hank you. I\\xe2\\x80\\x99m honored to be with you today for your commencement from one of the finest universities'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f38cfa-33ba-414f-cb29-2ee9226f069a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38aaecb1-05a3-46a0-cc60-3f64d8e535ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 69) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b42abd7-ef6d-41a8-8e46-5606f4806f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  17664     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  70725     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4026693 (15.36 MB)\n",
            "Trainable params: 4026693 (15.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2c6f86-dbe2-486b-b871-c99264e6f94f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39, 16, 68, 15, 11, 54, 30, 15, 58,  3, 53, 32, 38, 26, 42, 36, 37,\n",
              "       49, 52,  5, 67,  6, 37, 48, 51, 41,  7, 17, 42, 56, 18, 33, 42, 50,\n",
              "       46, 49, 68, 53, 63, 66, 58, 50, 56, 41, 26, 53, 24,  5,  9, 58, 46,\n",
              "       45, 64, 40, 59, 42,  8, 63, 39, 36, 23, 11, 16, 30, 50, 13,  6, 47,\n",
              "       21, 51, 34,  8, 16, 20, 44, 16, 52, 38, 43, 16, 60, 68, 22, 47, 28,\n",
              "       56, 65, 44,  1, 26,  5,  3, 18, 11, 12, 31, 40, 15, 22, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21f242b-9bdc-40ac-9358-638bf4d4c8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' was beautifully hand calligraphed. Because I had dropped out and didn\\xe2\\x80\\x99t have to take the normal cla'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Y9\\xe2\\x80\\x9d84oM8s nOWIcTVjm-\\xe2\\x80\\x9c.Vilb0:cq?Pckgj\\xe2\\x80\\x9dnx\\xe2\\x80\\x99skqbInG-2sgfyatc1xYTE49Mk6.hClR19Be9mWd9u\\xe2\\x80\\x9dDhKqze\\nI- ?45Na8Dp'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb1187f-022f-4827-ab83-ba82ff9564a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 69)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.2340627, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961c6f9a-c868-4733-8cef-ce9e54cead40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68.99697"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358bee8f-cfb9-4607-aada-c4698b652de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.2342\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 4.1862\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 4.1047\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 3.8158\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 9.0659\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 3.3424\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 1s 821ms/step - loss: 3.6426\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 3.6882\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 3.6307\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 3.5517\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 3.7484\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 3.5359\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 3.5231\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 3.5047\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 3.4734\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 3.4254\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 3.3593\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 3.2871\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 3.2401\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 3.1902\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 1s 829ms/step - loss: 3.1275\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 3.0720\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 3.0134\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 2.9874\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 3.0103\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 3.0291\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 3.0337\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 2.9899\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.9613\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 2.9543\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 2.9427\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 2.9298\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 2.9056\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 2.8910\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 2.8751\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 2.8584\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 637ms/step - loss: 2.8732\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 2.8313\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 2.8409\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 2.8365\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 2.8053\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 2.7885\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 2.7755\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 2.7596\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 2.7268\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 2.7273\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 2.6931\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6803\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 2.6895\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 2.6470\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 2.6367\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 2.6011\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 2.6009\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 2.5730\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 2.5653\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 2.5475\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 2.5482\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 2.5120\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 2.4856\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 2.5012\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4918\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 2.4635\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 2.4834\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 2.4464\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 2.4364\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 2.4518\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 2.4263\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 2.4056\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4128\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 2.4081\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 2.3888\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 2.3952\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 2.3884\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 2.3707\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 2.3624\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 2.3655\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 2.3565\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 2.3426\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 2.3351\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 2.3291\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 2.3352\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 2.3150\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 2.3207\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 2.2976\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 2.2997\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 2.2838\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 2.2879\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 2.2848\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 2.2987\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 2.2752\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 2.2592\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 2.2579\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 2.2647\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 2.2708\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 2.2345\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 2.2316\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 2.2240\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 2.2388\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 2.2213\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 2.1939\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b014b02-3193-4a21-c8dd-7700bec9a106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve:stir theveanmustogy I wofiso befe. Ato fom orourt one. I han jured ce whe ted tiney I ate titheap0rhat. 3utin. Ied onors thated tofele ut theo tet torer iyithar at as tha ceryrave koale pap bat ineogingnter eat byat roo becooul inserenpli ancyou. Whar fhe on hoveâ€™ts rom. Sreong eapl sid leas at if afail eveo Therich ang nife foullif an four tutimu de lin thy n ousy, ly lou fato ded the tha the have war socer ho and wasawaw wuth I fopo as wnes bevy hocrathe outt onecinatn yuaf ore, wa fe. Whet ent fessn y ot andrpagn prepla tiwkee in what om0 doonge, I I tour anes luod et thar terne thok the kl carot thed sf ill. Iteren wad cacu toligat dos tâ€™t aved yomwat whecto, fow it huy te wins le thaâ€™nd the Daat ontite d istut an, Yost alligeâ€™net to blot ast Neve thaathel iticge touly at pollinner sre Mof st curene ave hant autid0e hy amy ig sorype beise fured stop athe, I depasbe ye. 15 Nhak ceeletayto ngat wasry mur felok thelly anog be wiln lit coutid me befely bet ow erine bithy youd e. Sidere \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.347371578216553\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Steve:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('windows-1252'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1de4704-f5b1-4908-848a-c1ba80ed9b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Steve:Mlacicveat. It as. Whe fom, ththe..,. DSThe cout ifer moclaru an, creat. I inaf whour ant tithe, Burp dout I tore. dtici0 cat tour thor onishint mo pe dot open co go thind cad, I wime I wase cong, inne com of on anve nte se dan sot snd ougn ayoud thou low inppomy I lourse loullito fice tuthe ek fok cind it, lirery fouss ned tof irste bey hrte in he cole lhathing pisted atingr alvathyolove pply at futint en theasmes edolry, Shakand., I bhas fopom tos I jut r, at turdy soungo he men haen hhens oid lyan cand an thid for Siovyo beled ciat dol counsivisthat ines turezi\\xe2\\x80\\x99st ry comewase fof inpicg yowher 1vdac oupy sor to gnid Whel bo wan ce dom on an the yre lost of monlend y ever ithu wher icle andacl wicr als aw ou cithel fiwant aneal wast I weent op het pup gam if to f.\\xe2\\x80\\x99steat I thart at eat ifn torlreang so on wary af my anoud Ien soweid ya. I cou g that eapkef bed I domex toses wat in as at tour hoe Aofant thing daad I mout. Ned con thad bpat oudeshatot. Remane sigclal go loy On, tist fib'\n",
            " b'Steve:Ixt int wot womot ove oud got on toul. I ohtry yed andigry ponal youge hatienint lout voulle gaener ao ded ancely in tout fe\\xe2\\x80\\x99t ind andy, ton tou, ast. Iud thadile whivey ar out at. what diro. PMt You gre, yoit. To wall woulve sorece warp. Bowad wyure deaw, th\\xe2\\x80\\x99t d I Ctend Iy thavinined tookr tond to gso lod wive dad lat curin cat. youl if af thp ove sad gatekery0 yo cive knd, beve anthe an has the lreat gothe pf enere do and y fove\\xe2\\x80\\x99n entong meaved hond heos thay ifiste idno torent, we et folcey dor de walt ming founkiry lhe bersseclel ind heever camp heves t padking\\xe2\\x80\\x99. The hifisic dice wam\\xe2\\x80\\x99g\\xe2\\x80\\x9d Be ber, aczoflllged mot the Bin. Yechrokezeng o rer thitherid lat do orate by wathe, myty tor ig cetinntrosct me gisixrthy in yowhir mer yoKd be andounid wo hust ite cento an ing, gouk thand swro keacsteng in inled thang wyont innc, bef save aw cay aw the cayrest, tht yoingt co af nhat. I iteptat cinore putint the le andicillefe toce le inept ald it kest utiy6,,, I wrer theathe the theml yonthen tor'\n",
            " b'Steve:the songersing an pofe mhat iser afirha. Efterone. I rorpers ouat en dey thape bet Serury be rs oro fee af and ivere deabus dus et wal. ayd inife s ou int. I youl. Thed wurcond lf uve th tethe. Woun. It, ay malith inen hr oigppento hanow waene dlor ture the and eall ral oad \\xe2\\x80\\x9cfyoulVarg wathint, Whs, vto ibe\\xe2\\x80\\x99thecy einesl thede\\xe2\\x80\\x99s fure ind lo diw\\xe2\\x80\\x99rid fice\\xe2\\x80\\x99t. Thetonged tole id wast thco ker ofengeciny avest she chy fot ouded Ire cave wed a andet appes the sout anigint, I waraeco fe thabent therellid oug se yowes siul, the, Paalnt. Nete yed ind cropele. Non whare we fre diso wh len hertear soitpolliged, tred I yorellecine, deve tordurss hirstige fove yhoneat jug yo tirs at wis ihed the the thes t ong werseaned nt shis. Ye antent yo ber. Theomhin\\xe2\\x80\\x99s phit ag makte buxid the le nout fopou the ne pit y your ioedid verentand yory aifpbe tous c or dou cohe nif po higig. Tacs meroast shatoe? Steng nithew atertour. dhes m oos. I gpeatd rine soulleve whe tom mpals yom7 ne ve thiveped ratce cest d ave '\n",
            " b'Steve:Gif thavee soneverlinnd thlene y pas ot wre t anderading thyop, ar tomcecapt\\xe2\\x80\\x99l sofer. Ong ano all ppled one tyofuow afy wneitiscay oveen ald rer inethreas. Thadacher too heved pozimer llingod inof arllo sede her ipcrond s at tho gas depad tho asdinn llortirston Vaveferas tourissther mio halt y oo fer fer peaste au we ovend. Sheo botily aow yverene yompeongptist whave thane yor by beacsthop, atoivilistrastes foont irerered these lllicnllipe devit yfe s Wo gere mhng. I kag se kirh cound, I heed vevath touit outiwy as tsereror, wser they Ewan evintooching intor bat ife louvean\\xe2\\x80\\x99stye ber fat ineang Pactirg gour buned dul styethe thre tho hitha the th the lougrint thed s avengpl ond. Cre. The teor ast sat ralo be feare dout that groos. tha nt acle. She nntasge diwr thol on touthe bonng. I wrat weand yurasive koxe fout oullig\\xe2\\x80\\x99cn. thar,, Be rome paplls ced ture theas as leat ind watnifided life hrevetto. Stherssy am, ad toullsin coul thed foticivers fem versingtis. P2imyent incernepcal fome:Wh'\n",
            " b'Steve:fons fay y meint. Ang srow onone. Nool co g cant I oe o bat tole p, mobexive. Shnehy I ou a gee juedl iin af by woure beart wa diet worpo ge fome hentend\\xe2\\x80\\x99t lo aw sullyad wherte d wird ox popest ago mipongin wofinnd thet yor I dopisige westre ater muril nthet larded is crhe tosp no cead tos harent feve forate tasd ras the\\xe2\\x80\\x99s I ha byome isslly at amed yasld bols, doopd lith the lucisos al paly wit rerounthro sasn ry and molathu cery, hawhtis arasne you dillass to cane dus m ay piterned warDende dilut int ye anloy. I figh thert momed tha doy 3u goucered att ont dt Whelefing the bivering lkast watgrhen ter waris\\xe2\\x80\\x99e larne. We he byout conteay ay th theret I dow andopot rskel thaut thag rast toc mako, fow tosealis berewd tos thersu the ifut rea, brra mand ips th fourtuy the lalend yule blo ante, ou moud that cone har. Sho waat wang ong toury do wiwf olous wiveean y ary, the wat whowt ad delue\\xe2\\x80\\x99t oves sokne y af fined i for. In\\xe2\\x80\\x99d dod pive tharea0 ciale bud ware tholl -olwaan ind hn touths fope a'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.669386863708496\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Steve:', 'Steve:', 'Steve:', 'Steve:', 'Steve:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8aa8108-c992-493e-8d31-4dfd0dd10bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7855df2aa950>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1513347-80ee-453e-8c3c-f66f4d46c788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve:of barea moan doo warl’tur, aringe go lergalplivist wawat yt hat readdermat da put hogerhD. Deant avothe e woot gyet oo mo, pe. Sout Ma ferethe Senrllllderg thy af tha thel yuls, wad on th ou’st wow st ed acont thed round vor and be thalk Wery tapsy l’l Ba avet whe sowr aned ath dele datha’t casr, she the un wouthas tou tha gho madpte to d aveathe tuly I de tove me werit wryer ons call sowhe nereiEg st me he tors foutht ouse, Slom eacloned the bemy I doxtinery frtew inis wren ting arihakd oppere biint hae. zulnd, I canorirert ufd at cate th fount to pned po wo mise wary, dacige eveengastrey 19MvWce gells. It sis math myean dn nor ind nove. ging overt, Catidut yopfer I th cof lo stang Mo. I epnothe medeeve Benthut nathid coll ou cisty hew wht ionime ined piwad. Irrsit of the, I wha be as alirne theand th in list oug m ou warcy mopu belkou uty ourenr ghy at erond could have eatyer fighete d oroun, thas mawiee blunther foes yomed the srrathets dheint of me lavet yowe harim be, morste, I w\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['Steve:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9"
      },
      "outputs": [],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}